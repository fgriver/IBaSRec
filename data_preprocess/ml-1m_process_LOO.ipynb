{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Generate the corresponding Files about \"MovieLens-1M\"",
   "id": "1c2a25d13acb4c3a"
  },
  {
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2026-01-24T04:09:40.273883Z",
     "start_time": "2026-01-24T04:09:31.293291Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "\n",
    "# ==========================================\n",
    "# 1. 配置路径\n",
    "# ==========================================\n",
    "class Args:\n",
    "    raw_data_dir = '../raw_data'            # 数据集文件所在目录\n",
    "    processed_data_dir = '../dataset' # 输出目录\n",
    "    dataset = 'ml-1m'             # 数据集名称\n",
    "\n",
    "args = Args()\n",
    "dataset_dir = f\"{args.processed_data_dir}/{args.dataset}\"\n",
    "os.makedirs(dataset_dir, exist_ok=True)\n",
    "\n",
    "print(f\"Start processing {args.dataset}...\")\n",
    "\n",
    "# ==========================================\n",
    "# 2. 辅助函数 (文本清洗 & 迭代过滤)\n",
    "# ==========================================\n",
    "def norm_text(x):\n",
    "    \"\"\"标准化文本，过滤无意义字符\"\"\"\n",
    "    if x is None:\n",
    "        return None\n",
    "    if isinstance(x, float) and pd.isna(x):\n",
    "        return None\n",
    "    x = str(x).strip()\n",
    "    bad_tokens = {\"\", \".\", \"-\", \"--\", \"...\", \"N/A\", \"n/a\", \"None\", \"unknown\"}\n",
    "    if x in bad_tokens:\n",
    "        return None\n",
    "    # 如果全是符号则过滤\n",
    "    if all(not c.isalnum() for c in x):\n",
    "        return None\n",
    "    return x\n",
    "\n",
    "def iterative_filter(df, min_user=5, min_item=5):\n",
    "    \"\"\"递归过滤掉交互过少的用户和物品 (k-core)\"\"\"\n",
    "    changed = True\n",
    "    while changed:\n",
    "        changed = False\n",
    "        # ---- filter items ----\n",
    "        item_counts = df['asin'].value_counts()\n",
    "        valid_items = item_counts[item_counts >= min_item].index\n",
    "        new_df = df[df['asin'].isin(valid_items)]\n",
    "        if len(new_df) != len(df):\n",
    "            changed = True\n",
    "        df = new_df\n",
    "\n",
    "        # ---- filter users ----\n",
    "        user_counts = df['reviewerID'].value_counts()\n",
    "        valid_users = user_counts[user_counts >= min_user].index\n",
    "        new_df = df[df['reviewerID'].isin(valid_users)]\n",
    "        if len(new_df) != len(df):\n",
    "            changed = True\n",
    "        df = new_df\n",
    "    return df.reset_index(drop=True)\n",
    "\n",
    "# ==========================================\n",
    "# 3. 加载数据 (Movies & Ratings)\n",
    "# ==========================================\n",
    "print(\"Loading data...\")\n",
    "\n",
    "# 读取 movies.dat (Item Meta)\n",
    "# ML-1M format: MovieID::Title::Genres\n",
    "movies = pd.read_csv(\n",
    "    os.path.join(args.raw_data_dir, args.dataset, 'movies.dat'),\n",
    "    sep='::', engine='python', encoding='latin-1',\n",
    "    names=['asin', 'title', 'category']\n",
    ")\n",
    "\n",
    "# 清洗 Meta\n",
    "movies['title'] = movies['title'].apply(norm_text)\n",
    "movies['category'] = movies['category'].apply(norm_text) # 保留 Genres 字符串\n",
    "movies['description'] = None # ML-1M 无描述\n",
    "movies['asin'] = movies['asin'].astype(str) # 统一 ID 类型\n",
    "\n",
    "# 读取 ratings.dat (Interactions)\n",
    "# ML-1M format: UserID::MovieID::Rating::Timestamp\n",
    "ratings = pd.read_csv(\n",
    "    os.path.join(args.raw_data_dir, args.dataset, 'ratings.dat'),\n",
    "    sep='::', engine='python', encoding='latin-1',\n",
    "    names=['reviewerID', 'asin', 'rating', 'unixReviewTime']\n",
    ")\n",
    "ratings['reviewerID'] = ratings['reviewerID'].astype(str)\n",
    "ratings['asin'] = ratings['asin'].astype(str)\n",
    "ratings['unixReviewTime'] = ratings['unixReviewTime'].astype(int)\n",
    "\n",
    "# 初步过滤：只保留 Meta 中存在的 Item\n",
    "valid_asins = set(movies['asin'].values)\n",
    "ratings = ratings[ratings['asin'].isin(valid_asins)]\n",
    "\n",
    "print(f\"Raw ratings loaded: {len(ratings)}\")\n",
    "\n",
    "# ==========================================\n",
    "# 4. 执行过滤与 ID 映射\n",
    "# ==========================================\n",
    "print(\"Applying iterative filter...\")\n",
    "data_review_clean = iterative_filter(ratings, min_user=5, min_item=5)\n",
    "print(f\"Ratings after filter: {len(data_review_clean)}\")\n",
    "\n",
    "# 构建 ID 映射 (0 ~ N-1)\n",
    "unique_users = sorted(data_review_clean['reviewerID'].unique())\n",
    "unique_items = sorted(data_review_clean['asin'].unique())\n",
    "\n",
    "user2id = {u: i for i, u in enumerate(unique_users)}\n",
    "id2user = {i: u for i, u in enumerate(unique_users)}\n",
    "item2id = {a: i for i, a in enumerate(unique_items)}\n",
    "id2item = {i: a for i, a in enumerate(unique_items)}\n",
    "\n",
    "# 应用映射\n",
    "df_inter = data_review_clean.copy()\n",
    "df_inter['user_id'] = df_inter['reviewerID'].map(user2id)\n",
    "df_inter['item_id'] = df_inter['asin'].map(item2id)\n",
    "df_inter = df_inter[['user_id', 'item_id', 'unixReviewTime']]\n",
    "# 按用户和时间排序\n",
    "df_inter = df_inter.sort_values(['user_id', 'unixReviewTime']).reset_index(drop=True)\n",
    "\n",
    "# 保存映射文件\n",
    "for name, data in [('user2id', user2id), ('id2user', id2user),\n",
    "                   ('item2id', item2id), ('id2item', id2item)]:\n",
    "    with open(f\"{dataset_dir}/{name}.json\", \"w\") as f:\n",
    "        json.dump(data, f)\n",
    "\n",
    "# 保存 interactions.csv\n",
    "df_inter.to_csv(f\"{dataset_dir}/interactions.csv\", index=False)\n",
    "\n",
    "# ==========================================\n",
    "# 5. 保存处理后的 Item Meta (item_text.csv)\n",
    "# ==========================================\n",
    "final_valid_items = set(data_review_clean['asin'].unique())\n",
    "df_meta_clean = movies[movies['asin'].isin(final_valid_items)].copy()\n",
    "\n",
    "df_meta_clean['item_id'] = df_meta_clean['asin'].map(item2id)\n",
    "df_meta_clean = df_meta_clean.dropna(subset=['item_id'])\n",
    "df_meta_clean['item_id'] = df_meta_clean['item_id'].astype(int)\n",
    "\n",
    "# 整理列顺序\n",
    "df_meta_clean = df_meta_clean[['item_id', 'title', 'description', 'category']]\n",
    "df_meta_clean = df_meta_clean.sort_values('item_id').reset_index(drop=True)\n",
    "\n",
    "df_meta_clean.to_csv(f\"{dataset_dir}/item_text.csv\", index=False)\n",
    "print(\"Saved item_text.csv\")\n",
    "\n",
    "# ==========================================\n",
    "# 6. 数据集切分 (Leave-One-Out)\n",
    "# ==========================================\n",
    "print(\"Splitting dataset (LOO)...\")\n",
    "user2seq = {}\n",
    "for r in df_inter.itertuples():\n",
    "    user2seq.setdefault(r.user_id, []).append(r.item_id)\n",
    "\n",
    "train_rows, valid_rows, test_rows = [], [], []\n",
    "\n",
    "for user, seq in user2seq.items():\n",
    "    if len(seq) < 4: continue # 序列过短跳过\n",
    "\n",
    "    # Train: ... -> T-3 (Target: T-2)\n",
    "    train_rows.append([user, seq[:-3], seq[-3], len(seq[:-3])])\n",
    "    # Valid: ... -> T-2 (Target: T-1)\n",
    "    valid_rows.append([user, seq[:-2], seq[-2], len(seq[:-2])])\n",
    "    # Test:  ... -> T-1 (Target: T)\n",
    "    test_rows.append([user, seq[:-1], seq[-1], len(seq[:-1])])\n",
    "\n",
    "# 转换为 DataFrame\n",
    "cols = ['user_id', 'seq', 'next', 'len_seq']\n",
    "train_df = pd.DataFrame(train_rows, columns=cols)\n",
    "valid_df = pd.DataFrame(valid_rows, columns=cols)\n",
    "test_df  = pd.DataFrame(test_rows,  columns=cols)\n",
    "\n",
    "# 保存 .df (pickle)\n",
    "train_df.to_pickle(f\"{dataset_dir}/train_data.df\")\n",
    "valid_df.to_pickle(f\"{dataset_dir}/val_data.df\")\n",
    "test_df.to_pickle(f\"{dataset_dir}/test_data.df\")\n",
    "\n",
    "# ==========================================\n",
    "# 7. 生成统计信息 (_info.csv & data_statis.df)\n",
    "# ==========================================\n",
    "num_users = len(user2id)\n",
    "num_items = len(item2id)\n",
    "total_interactions = len(df_inter)\n",
    "sparsity = 1 - (total_interactions / (num_users * num_items))\n",
    "\n",
    "info = {\n",
    "    \"num_users\": num_users,\n",
    "    \"num_items\": num_items,\n",
    "    \"total_interactions\": total_interactions,\n",
    "    \"sparsity\": sparsity,\n",
    "    \"train_samples\": len(train_df),\n",
    "    \"valid_samples\": len(valid_df),\n",
    "    \"test_samples\": len(test_df)\n",
    "}\n",
    "\n",
    "pd.DataFrame([info]).to_csv(f\"{dataset_dir}/{args.dataset}_info.csv\", index=False)\n",
    "pd.DataFrame([{\"item_num\": num_items, \"user_num\": num_users}]).to_pickle(f\"{dataset_dir}/data_statis.df\")\n",
    "\n",
    "print(\"Processing Done!\")\n",
    "print(info)"
   ],
   "id": "2360374520568cff",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start processing ml-1m...\n",
      "Loading data...\n",
      "Raw ratings loaded: 1000209\n",
      "Applying iterative filter...\n",
      "Ratings after filter: 999611\n",
      "Saved item_text.csv\n",
      "Splitting dataset (LOO)...\n",
      "Processing Done!\n",
      "{'num_users': 6040, 'num_items': 3416, 'total_interactions': 999611, 'sparsity': 0.9515519584503, 'train_samples': 6040, 'valid_samples': 6040, 'test_samples': 6040}\n"
     ]
    }
   ],
   "execution_count": 5
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
