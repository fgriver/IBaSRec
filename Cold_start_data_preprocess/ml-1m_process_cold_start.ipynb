{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2026-01-24T04:40:29.323300Z",
     "start_time": "2026-01-24T04:40:29.319537Z"
    }
   },
   "source": [
    ""
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-24T09:02:26.276885Z",
     "start_time": "2026-01-24T04:48:23.478113Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "\n",
    "# ==========================================\n",
    "# 1. 配置路径与参数\n",
    "# ==========================================\n",
    "class Args:\n",
    "    raw_data_dir = '../raw_data'                 # 确保目录下有 movies.dat 和 ratings.dat\n",
    "    processed_data_dir = '../cold_start_dataset' # 输出到新文件夹\n",
    "    dataset = 'ml-1m'\n",
    "\n",
    "args = Args()\n",
    "dataset_dir = f\"{args.processed_data_dir}/{args.dataset}\"\n",
    "os.makedirs(dataset_dir, exist_ok=True)\n",
    "\n",
    "print(f\"Start processing {args.dataset} for Cold-Start (User-Split)...\")\n",
    "\n",
    "# ==========================================\n",
    "# 2. 辅助函数 (文本清洗 & 迭代过滤)\n",
    "# ==========================================\n",
    "def norm_text(x):\n",
    "    \"\"\"文本标准化，处理空值\"\"\"\n",
    "    if x is None: return None\n",
    "    if isinstance(x, float) and pd.isna(x): return None\n",
    "    x = str(x).strip()\n",
    "    bad_tokens = {\"\", \".\", \"-\", \"--\", \"...\", \"N/A\", \"n/a\", \"None\", \"unknown\"}\n",
    "    if x in bad_tokens: return None\n",
    "    return x\n",
    "\n",
    "def iterative_filter(df, min_user=5, min_item=5):\n",
    "    \"\"\"k-core 过滤：递归剔除交互太少的用户和物品\"\"\"\n",
    "    print(f\"Starting iterative filter (min_user={min_user}, min_item={min_item})...\")\n",
    "    changed = True\n",
    "    while changed:\n",
    "        changed = False\n",
    "        # Filter items\n",
    "        item_counts = df['asin'].value_counts()\n",
    "        valid_items = item_counts[item_counts >= min_item].index\n",
    "        new_df = df[df['asin'].isin(valid_items)]\n",
    "        if len(new_df) != len(df): changed = True\n",
    "        df = new_df\n",
    "\n",
    "        # Filter users\n",
    "        user_counts = df['reviewerID'].value_counts()\n",
    "        valid_users = user_counts[user_counts >= min_user].index\n",
    "        new_df = df[df['reviewerID'].isin(valid_users)]\n",
    "        if len(new_df) != len(df): changed = True\n",
    "        df = new_df\n",
    "\n",
    "    print(f\"Filter done. Rows: {len(df)}\")\n",
    "    return df.reset_index(drop=True)\n",
    "\n",
    "# ==========================================\n",
    "# 3. 加载数据 & 修复潜在报错\n",
    "# ==========================================\n",
    "print(\"Loading raw data...\")\n",
    "\n",
    "# ---- Load Movies (Meta) ----\n",
    "try:\n",
    "    movies = pd.read_csv(\n",
    "        os.path.join(args.raw_data_dir, args.dataset, 'movies.dat'),\n",
    "        sep='::', engine='python', encoding='latin-1',\n",
    "        names=['asin', 'title', 'category']\n",
    "    )\n",
    "except Exception as e:\n",
    "    raise FileNotFoundError(f\"读取 movies.dat 失败: {e}\")\n",
    "\n",
    "movies['title'] = movies['title'].apply(norm_text)\n",
    "movies['category'] = movies['category'].apply(norm_text)\n",
    "movies['description'] = None\n",
    "movies['asin'] = movies['asin'].astype(str)\n",
    "\n",
    "# ---- Load Ratings (Interactions) ----\n",
    "try:\n",
    "    ratings = pd.read_csv(\n",
    "        os.path.join(args.raw_data_dir, args.dataset, 'movies.dat'),\n",
    "        sep='::', engine='python', encoding='latin-1',\n",
    "        names=['reviewerID', 'asin', 'rating', 'unixReviewTime']\n",
    "    )\n",
    "except Exception as e:\n",
    "    raise FileNotFoundError(f\"读取 ratings.dat 失败: {e}\")\n",
    "\n",
    "ratings['reviewerID'] = ratings['reviewerID'].astype(str)\n",
    "ratings['asin'] = ratings['asin'].astype(str)\n",
    "\n",
    "# ★★★ 修复核心：安全转换时间戳，防止 NaN 导致的 IntCastingNaNError ★★★\n",
    "ratings['unixReviewTime'] = pd.to_numeric(ratings['unixReviewTime'], errors='coerce')\n",
    "# 如果有无法转化的行（NaN），直接丢弃\n",
    "ratings = ratings.dropna(subset=['unixReviewTime'])\n",
    "# 安全转为 int\n",
    "ratings['unixReviewTime'] = ratings['unixReviewTime'].astype(int)\n",
    "\n",
    "# 仅保留在 Meta 中存在的 Item\n",
    "valid_asins_meta = set(movies['asin'].values)\n",
    "ratings = ratings[ratings['asin'].isin(valid_asins_meta)]\n",
    "\n",
    "# 执行 K-core 过滤\n",
    "ratings = iterative_filter(ratings, min_user=5, min_item=5)\n",
    "\n",
    "# ==========================================\n",
    "# 4. ID Remapping (0 ~ N-1)\n",
    "# ==========================================\n",
    "print(\"Remapping IDs...\")\n",
    "unique_users = sorted(ratings['reviewerID'].unique())\n",
    "unique_items = sorted(ratings['asin'].unique())\n",
    "\n",
    "user2id = {u: i for i, u in enumerate(unique_users)}\n",
    "id2user = {i: u for i, u in enumerate(unique_users)}\n",
    "item2id = {a: i for i, a in enumerate(unique_items)}\n",
    "id2item = {i: a for i, a in enumerate(unique_items)}\n",
    "\n",
    "# 应用映射\n",
    "df_inter = ratings.copy()\n",
    "df_inter['user_id'] = df_inter['reviewerID'].map(user2id)\n",
    "df_inter['item_id'] = df_inter['asin'].map(item2id)\n",
    "df_inter = df_inter[['user_id', 'item_id', 'unixReviewTime']]\n",
    "# 按用户和时间排序\n",
    "df_inter = df_inter.sort_values(['user_id', 'unixReviewTime']).reset_index(drop=True)\n",
    "\n",
    "# 保存映射文件\n",
    "for name, data in [('user2id', user2id), ('id2user', id2user),\n",
    "                   ('item2id', item2id), ('id2item', id2item)]:\n",
    "    with open(f\"{dataset_dir}/{name}.json\", \"w\") as f:\n",
    "        json.dump(data, f)\n",
    "\n",
    "# 保存 interactions.csv\n",
    "df_inter.to_csv(f\"{dataset_dir}/interactions.csv\", index=False)\n",
    "\n",
    "# 保存 Item Text\n",
    "df_meta = movies[movies['asin'].isin(set(ratings['asin'].unique()))].copy()\n",
    "df_meta['item_id'] = df_meta['asin'].map(item2id)\n",
    "df_meta = df_meta.dropna(subset=['item_id'])\n",
    "df_meta['item_id'] = df_meta['item_id'].astype(int)\n",
    "df_meta = df_meta[['item_id', 'title', 'description', 'category']]\n",
    "df_meta = df_meta.sort_values('item_id').reset_index(drop=True)\n",
    "df_meta.to_csv(f\"{dataset_dir}/item_text.csv\", index=False)\n",
    "print(\"Base data processing done.\")\n",
    "\n",
    "# ============================================================\n",
    "# 5. Cold-start Split (User Split by Time) - 核心替换部分\n",
    "# ============================================================\n",
    "print(\"Splitting datasets (Cold-start User Split)...\")\n",
    "\n",
    "# 1. 计算每个用户的最后一次交互时间\n",
    "user_last_time = (\n",
    "    df_inter.groupby(\"user_id\")[\"unixReviewTime\"]\n",
    "    .max()\n",
    "    .reset_index()\n",
    "    .rename(columns={\"unixReviewTime\": \"last_time\"})\n",
    ")\n",
    "\n",
    "# 2. 按 last_time 从新到旧排序 (Newest Users first)\n",
    "user_last_time = user_last_time.sort_values(\"last_time\", ascending=False).reset_index(drop=True)\n",
    "\n",
    "num_users_total = len(user_last_time)\n",
    "n_test  = int(num_users_total * 0.1) # Top 10% users (Newest)\n",
    "n_valid = int(num_users_total * 0.1) # Next 10% users\n",
    "\n",
    "# 获取 User ID 集合\n",
    "test_users_set  = set(user_last_time.iloc[:n_test].user_id)\n",
    "valid_users_set = set(user_last_time.iloc[n_test:n_test+n_valid].user_id)\n",
    "train_users_set = set(user_last_time.iloc[n_test+n_valid:].user_id)\n",
    "\n",
    "print(f\"Split Result -> Train Users: {len(train_users_set)}, Valid Users: {len(valid_users_set)}, Test Users: {len(test_users_set)}\")\n",
    "\n",
    "# 3. 构建 user -> sequence 字典\n",
    "user2seq = {}\n",
    "# 使用 itertuples 加速遍历\n",
    "for r in df_inter.itertuples():\n",
    "    user2seq.setdefault(r.user_id, []).append(r.item_id)\n",
    "\n",
    "# 4. 生成数据集 DataFrame\n",
    "def build_dataset(user_set, mode='train'):\n",
    "    data_rows = []\n",
    "    for user in user_set:\n",
    "        seq = user2seq.get(user, [])\n",
    "        # 至少要有 1个历史 + 1个Target，否则没法预测\n",
    "        if len(seq) < 2:\n",
    "            continue\n",
    "\n",
    "        # Cold start split 通常取全量历史预测最后一个\n",
    "        # Input: seq[:-1] (0 to T-1)\n",
    "        # Target: seq[-1] (T)\n",
    "        hist = seq[:-1]\n",
    "        nxt = seq[-1]\n",
    "        data_rows.append([user, hist, nxt, len(hist)])\n",
    "\n",
    "    return pd.DataFrame(data_rows, columns=['user_id', 'seq', 'next', 'len_seq'])\n",
    "\n",
    "train_df = build_dataset(train_users_set, mode='train')\n",
    "valid_df = build_dataset(valid_users_set, mode='valid')\n",
    "test_df  = build_dataset(test_users_set,  mode='test')\n",
    "\n",
    "# 5. 保存 .df 文件\n",
    "train_df.to_pickle(f\"{dataset_dir}/train_data.df\")\n",
    "valid_df.to_pickle(f\"{dataset_dir}/val_data.df\")\n",
    "test_df.to_pickle(f\"{dataset_dir}/test_data.df\")\n",
    "\n",
    "print(f\"Samples generated - Train: {len(train_df)}, Valid: {len(valid_df)}, Test: {len(test_df)}\")\n",
    "\n",
    "# ==========================================\n",
    "# 6. 生成统计信息\n",
    "# ==========================================\n",
    "num_items = len(item2id)\n",
    "total_interactions = len(df_inter)\n",
    "seq_lens = [len(v) for v in user2seq.values()]\n",
    "sparsity = 1 - (total_interactions / (num_users_total * num_items))\n",
    "\n",
    "info = {\n",
    "    \"num_users\": num_users_total,\n",
    "    \"num_items\": num_items,\n",
    "    \"total_interactions\": total_interactions,\n",
    "    \"avg_seq_len\": round(np.mean(seq_lens), 3),\n",
    "    \"sparsity\": round(sparsity, 6),\n",
    "    \"train_samples\": len(train_df),\n",
    "    \"valid_samples\": len(valid_df),\n",
    "    \"test_samples\": len(test_df)\n",
    "}\n",
    "\n",
    "# 保存 info CSV\n",
    "pd.DataFrame([info]).to_csv(f\"{dataset_dir}/{args.dataset}_info.csv\", index=False)\n",
    "\n",
    "# 保存 data_statis.df (AlphaFuse/BetaFuse 代码可能需要)\n",
    "pd.DataFrame([{\"item_num\": num_items, \"user_num\": num_users_total}]).to_pickle(f\"{dataset_dir}/data_statis.df\")\n",
    "\n",
    "print(\"Processing Done!\")\n",
    "print(info)"
   ],
   "id": "24fe300cbc651cc2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start processing ml-1m for Cold-Start (User-Split)...\n",
      "Loading raw data...\n",
      "Starting iterative filter (min_user=5, min_item=5)...\n",
      "Filter done. Rows: 0\n",
      "Remapping IDs...\n",
      "Base data processing done.\n",
      "Splitting datasets (Cold-start User Split)...\n",
      "Split Result -> Train Users: 0, Valid Users: 0, Test Users: 0\n",
      "Samples generated - Train: 0, Valid: 0, Test: 0\n"
     ]
    },
    {
     "ename": "ZeroDivisionError",
     "evalue": "division by zero",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mZeroDivisionError\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[3], line 209\u001B[0m\n\u001B[1;32m    207\u001B[0m total_interactions \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlen\u001B[39m(df_inter)\n\u001B[1;32m    208\u001B[0m seq_lens \u001B[38;5;241m=\u001B[39m [\u001B[38;5;28mlen\u001B[39m(v) \u001B[38;5;28;01mfor\u001B[39;00m v \u001B[38;5;129;01min\u001B[39;00m user2seq\u001B[38;5;241m.\u001B[39mvalues()]\n\u001B[0;32m--> 209\u001B[0m sparsity \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m \u001B[38;5;241m-\u001B[39m (\u001B[43mtotal_interactions\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m/\u001B[39;49m\u001B[43m \u001B[49m\u001B[43m(\u001B[49m\u001B[43mnum_users_total\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mnum_items\u001B[49m\u001B[43m)\u001B[49m)\n\u001B[1;32m    211\u001B[0m info \u001B[38;5;241m=\u001B[39m {\n\u001B[1;32m    212\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnum_users\u001B[39m\u001B[38;5;124m\"\u001B[39m: num_users_total,\n\u001B[1;32m    213\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnum_items\u001B[39m\u001B[38;5;124m\"\u001B[39m: num_items,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    219\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtest_samples\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;28mlen\u001B[39m(test_df)\n\u001B[1;32m    220\u001B[0m }\n\u001B[1;32m    222\u001B[0m \u001B[38;5;66;03m# 保存 info CSV\u001B[39;00m\n",
      "\u001B[0;31mZeroDivisionError\u001B[0m: division by zero"
     ]
    }
   ],
   "execution_count": 3
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
